Guest lecture:

1 - BARCO

What was it about?

Projectors, conference room garbage (clickshare)

They used large scale agile framework.

Architecture: Base unit + sub modules

Security vs Testing -> Security wants no access to things while testing does the opposite. So we want to have as much access to test.

A company that started from manual and eventually went to automated. 

They went from test team -> scrum teams with one tester. Feature and product owner for each team and mostly automated.

Requirements are made based on software test not predefined requirements. So if changes need to happen in requirements, they happen through test instead of requirement list. 

Went from reverse V-model to hourglass to vulcanic model. Now 95% is automated compared to before (where there was nothing).

Two levels of testing: Machine mimicing and hardware test.

What was good and what could go better?
+ Switch to agile dev, testers included in development. 
+ A lot of it is automated.
+ Test are split in stages.
+ Teams are responsible for there own product (product owner) 

- Test are seen as requirements, not requirements as basis

2 - OMP

Supply chain management / planning

System is based on c++ and QT. Frontend React / Typescript.

Unit test made with C# XUnit. White box testing where we know the internal code.

Unit test for frontend was made using Jest. They used snapchot testing, where they compare outputs of different versions. Downside of this, small changes could reflect change in output, and result in failure of test.

Contract test: API testing using postman, automating api workflow. Easier for less code proficient tester.

Selenium end to end test for webui. Testing ui based things could be flaky, because of the waiting concept. Where we need to wait for certain components to load or interact and could be different every time. Making the test fail at some times and pass at others.

End to end testen, they used squish. easy to start, hard for complex shit

customer application testing, they provide an external rest api to test the additional features a customer adds. (only if the customer really uses it)

Code reviews, tests are also reviewed by fellow coders, they act as an documentation for the code. manual test before merge.

stage based deployment

test driven dev, it helps well by fixing bugs.

risk based testing better than the amount of tests written

High-level modules should not depend on low-level modules. Both should depend on abstractions.
Abstractions should not depend on details. Details should depend on abstractions. -> DIP

What is good and bad?

+ They make use of many types of testing, not one fits all.
+ they make use of test driven dev, so regression testing is kinda already there
+ Customer can write their own tests

- Snapchot tests are not perfect
- Flaky test in selenium

3 - mediagenix

Streaming bs

They made use of multiple platforms. They explained about how testing infrastructure looks like and what it ideally looks like compared to what actually is used in practice.

release of product is different for every product, one releases multiple times a year while some other every so often.

depending on what needs to be fixed/upgraded/added the code repo might change. some could affect the base and some might complement.

They have one QA person for every squad. There are more teams for customer application then base code. They made use of small talk to implement there test because there code was in small talk. QA makes the test in gherkin scripts while dev makes it in small talk.

They have many manual tests. But are they all still valid, is unknown. use gherkin scripts, a user stories based testing. there are several issues with manual testing.

Custom implementations causes, many different versions, leading to increased maintenance. 

The new customers cant add custom features anymore.

What is good or bad?
+ They have CI
+ Went from custom features to no custom features anymore

- Many different versions already exists, so still has to be maintained even if they don't allow custom features anymore. Also many platforms.
- CI Is complicated
- Custom features leads to failing test
- Many manual decision and testing

4 - Cypress

Testing tool for frontend

Human make mistakes, that is why we want to use a tool.

Drag and drop, low code tool. 

Modern, free

it is complex. slow to create test, because decent code takes time

easy to get lost in the code, it might look jibberish after a while

Page object model -> define a page before hand instead of retyping every button or refetching the component. This makes the code more clear and more readable. -> Reusabillity. 

Do everything or make rules on what to define as a component and what not.

Mocking api respones: api will be actually be sent but cypress will make a mock response. (backend not tested with this)

5 - M2Q

Inventory managment, warehouse management.

risk assesment with business value in mind, they looked at how fast new features were developed and how much risk with how much business value.

It is a project that started from scratch.

In early stages they could develop a lot of features with little risk. Later on this slowed down because the amount of test needed to be devloped increased, this is because the quality standards increased.

In early stages there were barely any quality assurance, it was more of a feature than a must.

Validations were done in real time because the development was on site. so fast feedback loop. 

development happened for multiple services, that were stand alone and needed to be integrated.

Integration slowed down because of erp, and http.

Multiple styles of testing:
- e2e func test
- user accepting testing
- contract testing for integration

They introduced staging, to slow down development so end users can verify the new features.

Now they also have a release procedure, to garantue running production

and new features needs to be planned over a bigger scale.

Use what is there, so employees with existing knowledge. 

QA is not set on stone, adapt if needed

6 - Cegeka

Cegaka - consulted company

They also talked abt that bigger companies can not release a new product every sprint, this is mainly for startups. That small changes can impact many things.

some teams worked in a weird agile concept but it was basically waterfall. 

project management: 
- jira
- azure dev

used 2 different apps

Test cases were written everywhere, in notes, jira, even not physically (so in peoples head)

look at existing knowledge and market to determine what is the best tool to use

Merge everything to a singular tool

Ideal world -> pyramid but what they had was more of a hourglass.

choose tool and look at pros and cons between each

4 questions?
- who
- what
- infrastructure
- extra

Ranorex is code less tool, you create tests based on the recordings -> no technical background

proof of concepts with chosen tool if it is actually good

the bigger the company, the longer it takes

phase 1 - training - 1/2 days a week coaching - training new teams - coaching - ...

looking at the test cases for regression test, it was really small amount of tests

devs had too much work, so they could not make test in that amount of time

phase 2 - accepot and adapt

create a team: test automation experts -> support other teams
-> create tests for them
-> different tool -> test automators instead of no coders

Negative: team of test automation, make new back log but different teams runs the test and maintains it. Instead of one team doing both 
Negative: the seperate team now also needs to understand code they didnt work on

workflow, how to work with other teams for test auto
- backlog of scripts needed to autommate
- estimate how long
- when

work in a iterative way

C# testing framework
- they make use of a common framework for all teams

They went from little to no test to a lot of test

Unique approach for unique customer

7 - SQAI

Using AI to write test

many benifits:
- time
- less personal
- cost

Using multiple LLMs for different use cases, so the right tasks is asked to the right LLM

Heavy lifting by AI, validated by human

Humans for critical analysis

Many real life use cases, this helps normal testers because it makes use things the tester should already understand.

Mainly to complement humans not replace.

8 - redwire

satelite company, safety critical

architecture is made of different components

there designs need to comply to external standards (goverment)

standards are mainly for engineering and product assurance

They make use of different life cycle model, sequential models, iterative and multi level

They also make use criticality analysis to determine severity if something fails

sw validation will be done in multipe stages:
- unit test
- integration test
- validation test

Secure coding - coding so it is resistant to security vulnerability

FDIR
- How to detect failures
- how to isolate failures
- how to recover from these failures

9 - fluvius

Precise work because the network needs to be stable

Pays others to use energy to stabilize network

They write emergency scripts for in case something went wrong.

Loose requirements

Focus on reuse code, because code that is already considered safe is still safe. Only one chance to be right.

Unit tests written in python, made with ai and tested auto

system test are scenario driven. code as black box

integration test, see how good sequences work and if they work right

end to end testing, real life test with customers

experts say 10 reductions, so we only do 2 to be safe

10 - Siemens

digital twin, to perform testing

Safety critical 

test types:
System in the loop
human in the loop
Rapid control prototyping 
hardware in the loop

Digital twin itself is a form of testing

Multiple interpretation of one design - that is why requirements need to be defined properly, so measurable requirements. Specify test setup, test execution and teardown for these requirements

you need to start from some specifications to start testing

Unit testing - reviewed by other dev 
Integration testing - also reviewed and runned
end to end - also reviewed and runned, run in ci
performance testing - when something might be wrong
factory testing - before to customer - specific setup for customer (special case)
customer integration testing - on site at the customer
exploraty testing - manual test to find new errors - montly - using mutation testing and fuzzer for rest api

many things a tester should take in to account when testing. it is not just checking if the system works.

11 - colruyt

works really intern, so all the support for it mainly goes to them

tells me about what quality is in a round a bout way

quality is subjective, based on customer standards

Quality is often defined by the promise we make to our customers -> business value

makes use of standards to ensure quality 

quality attributes with the expectations of the customer

any test that exist, needs to have some business value

Most useless guest lecture

